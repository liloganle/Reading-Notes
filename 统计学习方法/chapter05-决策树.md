# 第五章　决策树

决策树是一种基本的分类与回归方法。这里只讨论用于分类问题的决策树模型。在分类问题中，决策树表示基于特征对输入的样本进行分类的过程。其主要优点是模型具有可读性、分类速度快。学习时，利用训练数据集，根据损失函数最小化的原则建立决策树模型；预测时，利用该模型对新的数据进行分类。决策树学习过程通常包含三个阶段：**特征选择**、**决策树生成**和**决策树剪枝**。

## 5.1 决策树模型及其学习

分类决策树模型是一种描述对样本进行分类的树形结构。它是由节点和有向边组成的。节点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类别。

决策树对输入样本进行分类时，先从根节点开始，对该样本的某一特征进行测试，依据测试结果，将样本分配到相应的子节点上；每个子节点对应着该特征的一个取值。如此递归地对样本进行测试，直至叶节点。这样就完成了分类过程。决策树即可以表示成一个if-then规则结合，也可以表示在给定特征条件下类的条件概率分布。

决策树学习的本质是从训练数据集中归纳出一组分类规则。我们所需要的是一个与训练数据集矛盾较小且具有很好的泛化性能的决策树。由于决策树可以表示为一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型。决策树生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。常用的决策树算法有$ID3$，$C4.5$，$CART$。

## 5.2 特征选择

特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。通常特征选择的准则是**信息增益**和**信息增益比**。

信息增益表示已知特征的信息，使得类信息不确定性减少的程度。一般地，熵与条件熵之差称为**互信息**。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。

以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。为了校正这一问题，使用信息增益率。它是训练数据集中信息增益与特征的熵之比。

## 5.3 决策树生成

### 5.3.1 $ID3$算法

$ID3$算法的核心是：在决策树各个节点上使用信息增益作为特征选择的准则，递归的构建决策树。具体方法如下：

- 首先从根节点开始，计算所有可能特征的信息增益，选择信息增益最大的特征作为根节点的特征，由该特征的不同取值构建子节点；
- 再对子节点递归的调用上一步骤方法，来构建决策树；
- 直到所有特征的信息增益均很小或是没有特征选择为止，这样就得到了决策树模型。

$ID3$算法相当于利用了极大似然法来进行概率模型的选择。

### 5.3.2 $C4.5$算法

$C4.5$算法是在$ID3$算法的基础上进行的改进版本，该算法使用了信息增益比来作为特征选择的准则。具体生成方法与$ID3$类似。

## 5.4 决策树剪枝

决策树生成算法一直递归的迭代下去，很有可能会出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据正确分类，从而构造出了过于复杂的决策树。为了解决这个问题，考虑对已经生成的决策树进行简化，这个过程称为剪枝。具体而言，就是从已生成的决策树上裁剪掉一些子树或叶节点，并将其根节点或父节点作为新的叶节点，从而简化模型。

可以看出，决策树生成只考虑了提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化模型，减少模型复杂度。因此决策树生成学习局部模型，而决策树剪枝学习整体模型。

## 5.5 $CART$算法

分类与回归树（$CART$）算法是应用最广泛的决策树算法。该算法同样由特征选择、树生成以及剪枝组成，既可以用于分类也可以用于回归。

$CART$算法由以下两个步骤组成：

1. 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；
2. 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时可用损失函数最小或者准确率提高作为剪枝的标准。

决策树的生成就是递归的构建二叉树的过程。对于回归树用均方误差最小化准则来选择特征，对于分类树用基尼指数最小化准则来选择特征。在剪枝得到的所有子树序列通过交叉验证选取最优的决策树。

