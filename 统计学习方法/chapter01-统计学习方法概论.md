# 第一章　统计学习方法概论

## 1.1 统计学习

统计学习是一门关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的学科，也称为统计机器学习。它的目的是对数据进行预测和分析，特别是对未知的新数据进行预测和分析。统计学习方法主要由**监督学习**、**无监督学习**、**半监督学习**以及**强化学习**组成，它一般包括统计学习方法、统计学习理论和统计学习应用三个方面。统计学习方法的研究旨在开发新的学习方法；统计学习理论的研究在于探求统计学习方法的有效性与效率，以及统计学习的基本理论问题；统计学习应用的研究主要考虑将统计学习方法应用到实际问题中去，解决实际问题。

## 1.2 监督学习

监督学习是统计学习中极其重要的一个分支，同时也是统计学习中内容最丰富、应用最广泛的部分。

## 1.3 统计学习三要素

统计学习方法是由**模型**、**策略**和**算法**这三大要素构成的。

### 1.3.1 模型

统计学习首先考虑的是学习什么样的模型。在监督学习过程中，模型就是所要学习的条件概率分布$P(Y|X)$或决策函数$Y=f(X)$。

### 1.3.2 策略

统计学习接着需要考虑的是按照什么样的准则学习或者选择最最优的模型。统计学习的目标在于从假设空间中选取最优模型。

在这里引入**损失函数**和**风险函数**。损失函数是度量模型一次预测的好坏，风险函数是度量平均意义下模型预测的好坏。损失函数是用来度量预测错误的程度，其值越小，模型就越好。统计学习中常用的损失函数有：

- 0－1损失函数
- 平方损失函数
- 绝对损失函数
- 对数损失函数（对数似然损失函数）

损失函数关于联合概率$P(X,Y)$的期望，称为风险函数或者**期望风险**（**期望损失**）。学习的目标就是选择期望风险最小的模型，然而由于联合概率分布$P(X, Y)$未知，所以期望风险不能直接计算。由此引入**经验风险**（**经验损失**），它是模型损失函数关于训练数据集的平均。根据大数定律，当训练样本数量趋于无穷时，经验风险趋于期望风险。所以可以用经验风险来估计期望风险。由于现实中训练样本数量有限，所以用经验风险估计期望风险常常并不理想，需要对经验风险进行校正。由此引入监督学习的两个基本策略：**经验风险最小化**和**结构风险最小化**。

经验风险最小化策略认为，经验风险最小的模型就是最优的模型。当样本数量足够大时，经验风险最小化能够保证有很好的效果，所以在现实中被广泛的采用。*极大似然估计*就是经验风险最小化的一个例子。但若是样本数量很少，该策略的学习会产生“过拟合”现象。为了防止过拟合就提出来了结构风险最小化策略，该策略实在经验风险的基础上加上表示模型复杂度的**正则化项**（**惩罚项**），用**正则化项系数**来平衡经验风险和模型复杂度。结构风险最小化策略认为，结构风险最小的模型是最优的模型。

### 1.3.3 算法

算法是指学习模型的具体计算方法。我们基于训练数据集，根据学习策略，从假设空间中选取最优模型，最后考虑采用什么样的计算方法来求解最优模型。这就将学习问题转化为最优化问题了。

## 1.4 模型评估与模型选择

统计学习的目的是，使得学到的模型不仅对已知数据有很好的预测能力，而且对未知的数据有很好的泛化能力。

### 1.4.1 训练误差和测试误差

**训练误差**是模型关于训练数据集的平均损失；**测试误差**是模型关于测试数据集的平均损失。当损失函数确定时，基于损失函数的模型训练误差和模型测试误差就成为了学习方法的评估标准。注意，统计学习方法具体采用的损失函数未必是评估时使用的损失函数，但是让两者一致是比较理想的。

### 1.4.2 过拟合与模型选择

如果我们一味地追求提高对训练数据集的预测能力，那么我们所选择的模型复杂度往往要比真实模型更高，使得学习到的模型对已知的数据预测得很好，但对于未知数据预测得很少，这种现象称为**过拟合**。模型的选择旨在避免过拟合现象并提高模型的预测能力。即选择复杂度恰当的模型，以达到使得测试误差最小的学习目的。

## 1.5 正则化与交叉验证

### 1.5.1 正则化

**正则化**是结构风险最小化策略的实现，是在经验风险的基础上加上一个正则化项（惩罚项）。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项值就越大。正则化项可以是模型参数向量的范数，采取不同的形式。正则化的作用是选择经验风险与模型复杂度同时较小的模型。

### 1.5.2 交叉验证

常用模型选择的方法是**交叉验证**。如果给定的样本数据足够多，可以随机地将数据集划分为训练集、验证集和测试集三部分。训练集用来训练模型，验证集用于模型的选择，而测试集则用于对学习方法的评估。在所有学习到的不同复杂度的模型中，选择在验证集中预测误差最小的模型。由于数据足够多，因此用它对模型进行选择是有效的。

在实际应用中，许多时候数据是不充足的，为了选出好的模型，可以采用交叉验证的方法，其基本思想是重复地使用数据，主要有以下三种方法：

- 简单交叉验证
- $S$折交叉验证
- 留一交叉验证

## 1.6 泛化能力

学习方法的泛化能力是指，该学习方法学到的模型对未知数据的预测能力，它是学习方法本质上重要的性质。在现实中，采用最多的办法是通过测试误差来评价学习方法的泛化能力。但是该评价是依赖于测试数据集。事实上，泛化误差就是所学到模型的期望误差。

## 1.7 生成模型与判别模型

监督学习可以分为**生成式方法**和**判别式方法**。所学到的模型分别称为生成式模型和判别式模型。生成式方法是由数据学习联合概率密度$P(X,Y)$，然后求出条件概率分布$P(Y|X)$，典型的生成式模型有：朴素贝叶斯方法和隐马尔科夫模型。判别式方法是由数据直接学习决策函数$f(X)$或条件概率分布$P(Y|X)$，典型的判别式模型包含：$k$近邻方法，感知机，决策树，逻辑回归，最大熵模型，支持向量机，集成方法和条件随机场等。

## 1.8 监督学习三大重要问题

1. 分类问题：是监督学习的一个核心问题。该问题对应的模型称为分类器，评价分类器性能的指标一般是精确率、召回率和$F_1$值。
2. 标注问题：可以认为标注问题是分类问题的一个推广，标注问题又是一个更复杂的结构预测问题的简单形式。标注问题常用的统计学习方法有：隐马尔科夫模型和条件随机场。
3. 回归问题：回归问题是监督学习的另一个重要问题。回归问题的学习等价于函数拟合，即选择一条函数曲线使其很好地拟合已知数据且很好的预测未知数据。

注意：在之后的章节中，概率$P(X=x,Y=y)$和$P(Y=y|X=x)$有时会缩写成如$P(x,y)$和$P(y|x)$。