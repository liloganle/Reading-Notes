# 第三章　$k$近邻算法

$k$近邻算法是一种基本的分类与回归算法。这里只讨论分类问题中的$k$近邻算法。它实际上是利用了训练数据集对特征向量空间进行划分，并作为其分类结果的模型。$k$值的选取、距离度量以及分类决策规则是$k$近邻算法的三个基本要素。该算法没有显示的学习过程。

## 3.1 $k$近邻算法

该算法的实现步骤：

- 输入：训练数据集$T​$
1. 根据给定的距离度量，在训练集$T​$中找到与$x​$距离最近的$k​$个样本；
2. 根据分类决策规则，找出$k$个样本中多数的类别$y$，那么就判定未知类别样本$x$的类别为$y$。
- 输出：样本$x$所属的类别$y$

注意，当$k=1$时，则该算法称为**最近邻算法**。

## 3.2 $k$近邻模型的三个基本要素

### 3.2.1 距离度量

特征空间中两个样本的距离是它们相似程度的反映。距离度量既可以是**欧氏距离**，也可以是更一般的**$L_{p}$距离**或者**明式距离**。假设特征空间$\mathcal{X}$的维数是$n$维，那么对于$x_i, x_j \in \mathcal{X}$，它们的**$L_{p}$距离**或者**明式距离**为：
$$
L_p(x_i,x_j)=[\sum_{l=1}^n |x_i^{(l)} -x_j^{(l)}|^{p}]^{\frac{1}{p}}
$$
当$p=1$时，则称为曼哈顿距离；当$p=2$时，则称为欧氏距离；当$p=\infty$时，则称为切比雪夫距离，它是各个坐标距离的最大值。注意，*不同的距离度量所确定的最近邻点是不同的*。

### 3.2.2 $k$值的选择

$k$值的选择会对$k$近邻算法的结果产生重大的影响。如果选取的值较小，就相当于用较小的邻域中的训练样本来进行预测，“学习”的**近似误差**会减少，只有与输入样本相似的训练样本才会对预测结果起作用。同时它的缺点是“学习”的**估计误差**会增大，预测结果会对邻近的样本十分敏感。如果邻近的样本恰好是噪声，那么预测就会出错。总的来说，$k$值的减少会导致整体的模型变得复杂，容易发生过拟合。

如果选取的$k$值较大，就相当于用较大的邻域中的训练样本来进行预测。其优点是可以减少估计误差，但缺点是近似误差会增大。这时与输入样本距离较远的训练样本也会对预测起作用，使得预测可能发生错误。

在实际应用中，$k$值一般是选取一个较小的数值。通常采用交叉验证的方法来选取最优的$k$值。

### 3.2.3 分类决策规则

在$k$近邻算法中，分类决策规则经常是**多数表决**，即输入样本的$k$个邻近的训练样本中多数类别决定输入样本类别。因为多数表决规则等价于经验风险最小化。

## 3.3 $kd$树

实现$k$近邻算法主要考虑的问题是如何对训练数据进行快速的$k$邻近搜索。这一点在特征空间维数大以及训练数据集数量大时尤其重要。最简单的实现方法是**线性扫描**。它需要计算输入样本与每个训练样本的距离，当训练数据集很大时，计算是非常耗时的。因此为了提高算法的效率，可以考虑使用特殊的结构存储数据，以减少计算距离的次数。具体方法有很多，以$kd$树方法为代表。

通常，$kd$树依次选坐标轴对空间进行划分，划分点选定训练样本在该坐标轴上的中位数作为切分点，这样得到的$kd$树是平衡的。注意，平衡的$kd$树搜索时的效率未必是最优的。$kd$树更适合用于训练样本数远大于空间维数时的$k$近邻搜索。当空间维数接近训练样本数的时候，它的效率会迅速下降，几乎接近线性扫描。

