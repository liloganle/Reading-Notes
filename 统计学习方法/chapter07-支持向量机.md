# 第七章　支持向量机

支持向量机是一种二分类模型。它的基本模型是定义在特征空间上的**最大间隔线性分类器**，间隔最大使它有别于**感知机**。支持向量机还包括核技巧，这使它能够成为实质上的**非线性分类器**。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划问题；它的学习算法就是求解凸二次规划的最优化算法。

当输入空间为欧式空间或者离散集合、特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，其等价于隐式地在高维空间中学习线性支持向量机，这样的方法称为核方法。核方法是比支持向量机更为一般的机器学习方法。

## 7.1 线性可分支持向量机与硬间隔最大化

（**线性可分支持向量机**）给定线性可分训练数据集，通过硬间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为：
$$
w^{*}\cdot x+b^{*}=0
$$

以及相应的分类决策函数为：
$$
f(x)=\text{sign}(w^{*}\cdot x + b^{*})
$$
的支持向量机称为线性可分支持向量机。

（**函数间隔**）对于给定的训练数据集$T$和超平面$w\cdot x +b =0$，定义超平面关于样本点（$x_i, y_i$）的函数间隔为：
$$
\hat{\gamma}_i = y_i (w\cdot x_i + b)
$$
定义超平面关于训练数据集$T$的函数间隔为超平面关于$T$中所有样本点（$x_i, y_i​$）的函数间隔中的最小值，即
$$
\hat{\gamma} = \min_{i=1,2,\cdots,N}\hat{\gamma}_i
$$

（**几何间隔**）对于给定的训练数据集$T$和超平面$w\cdot x + b = 0$，定义超平面关于样本点（$x_i, y_i$）的几何间隔为：
$$
\gamma_i = y_i(\frac{w}{\Vert w\Vert}\cdot x_i + \frac{b}{\Vert w\Vert})
$$
定义超平面关于训练数据集$T$的几何间隔为超平面关于$T$中所有样本点（$x_i, y_i$）的几何距离中的最小值，即
$$
\gamma = \min_{i=1,2,\cdots, N}\gamma_i
$$
从函数间隔和几何间隔的定义可知，函数间隔与几何间隔有如下的关系：
$$
\gamma_i = \frac{\hat{\gamma}_i}{\Vert w \Vert} \\
\gamma = \frac{\hat{\gamma}}{\Vert w \Vert}
$$
如果$\| w\|=1$，那么函数间隔就等于几何间隔。

**间隔最大化**的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据样本进行分类。即可以表示如下约束优化问题：
$$
\max_{w,b}\quad \gamma \\
\text{s.t.}\quad \gamma_i \geq \gamma, \quad i=1,2,\cdots,N
$$
依据上面的表示，可将该约束优化问题转化为一个等价的最优化问题：
$$
\max_{w,b}\quad \frac{\hat{\gamma}}{\Vert w\Vert} \\
\text{s.t.}\quad y_i(\frac{w}{\Vert w\Vert}\cdot x_i + \frac{b}{\Vert w\Vert})\geq \frac{\hat{\gamma}}{\Vert w\Vert}, \quad i=1,2,\cdots,N
$$
由于函数间隔$\hat{\gamma}$的取值大小并不影响最优化问题的求解。因此取$\hat{\gamma}=1$代入上面的最优化问题，则可以转换为：
$$
\min_{w,b}\quad \frac{1}{2}\Vert w\Vert^2 \\
\text{s.t.}\quad y_i(w\cdot x_i +b )\geq 1,\quad i=1,2,\cdots,N
$$
（**最大间隔分离超平面的存在唯一性**）若训练数据集$T$线性可分，则可将训练数据集中的样本完全正确分类的最大间隔分离超平面存在且唯一。

利用拉格朗日乘子法可得拉格朗日函数：
$$
L(w,b,\alpha)=\frac{1}{2}\Vert w\Vert^2+\sum_{i=1}^{N}\alpha_i [1-y_i(w\cdot x_i + b)]，\quad \alpha_i \geq 0
$$
将拉格朗日函数分别对$w,b$求偏导并令其等于$0$，求得：
$$
w = \sum_{i=1}^{N}\alpha_i y_i x_i \\
\sum_{i=1}^{N}\alpha_i y_i=0
$$
将此结果代入上述拉格朗日函数，即得：
$$
L(w,b,\alpha)=-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i \alpha_j y_i y_j(x_i\cdot x_j)+ \sum_{i=1}^{N}\alpha_i
$$
那么原问题的对偶问题为：
$$
\max_{\alpha}\quad -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i \alpha_j y_i y_j(x_i\cdot x_j)+ \sum_{i=1}^{N}\alpha_i \\
\text{s.t.}\quad \sum_{i=1}^{N}\alpha_i y_i=0 \\
\qquad \alpha_i\geq 0,\quad i=1,2,\cdots,N
$$
只要求解得到对偶问题的解$\alpha^{*}$，那么依据KKT条件可以求得原问题的解为：
$$
w^{*}=\sum_{i=1}^{N}\alpha_i^{*}y_i x_i \\
b^{*}=y_j-\sum_{i=1}^{N}\alpha_{i}^{*}y_i(x_i\cdot x_j)
$$
其中，下标$j​$使得$\alpha_{j}^{*} > 0​$。

训练数据集线性可分是理想的情况。在现实问题中，训练数据集往往是线性不可分的，即在样本中存在噪声或者异常样本。

## 7.2 线性支持向量机与软间隔最大化

线性可分支持向量机学习方法对线性不可分训练数据集是不适用的，因为这时上述方法中的不等式约束（$y_i(w\cdot x_i + b)\geq 1$）并不都成立。因此需要修改硬间隔最大化，使其成为软间隔最大化。

训练数据集之所以线性不可分，是因为其中某些样本无法满足其函数间隔大于等于１的约束条件。为了解决这个问题，对数据集的每个样本点（$x_i, y_i​$）引入一个松弛变量$\xi_i​$，且$\xi_i \geq 0​$，使函数间隔加上松弛变量后大于等于１。那么线性不可分的线性支持向量机的学习问题就转变为如下的凸二次规划问题：
$$
\min_{w,b,\xi}\quad \frac{1}{2}\Vert w\Vert^2 + C\sum_{i=1}^{N}\xi_i \\
\text{s.t.}\quad y_i(w\cdot x_i + b)\geq 1-\xi_i,\quad i=1,2,\cdots,N \\
\xi_i \geq 0,\quad i=1,2,\cdots,N
$$
其中，$C>0$称为惩罚参数，一般是由应用问题决定。$C$值越大时，对误分类的惩罚增大；$C$值变小时，对误分类的惩罚减少。

凸二次规划的解是存在的，并且可以证明$w$的解是唯一的，但$b$的解可能不唯一，而是存在于一个区间。依据KKT条件，同理可解得：
$$
w^{*}=\sum_{i=1}^{N}\alpha_i^{*}y_i x_i \\
b^{*}=y_j-\sum_{i=1}^{N}\alpha_{i}^{*}y_i(x_i\cdot x_j)
$$
其中，下标$j$使得$0<\alpha_{j}^{*}<C$。

软间隔的支持向量$x_i$要么在间隔边界上，要么在间隔边界与分离超平面之间，要么在分离超平面误分类一侧。

## 7.3 非线性支持向量机与和函数呢

对于解决线性分类问题，线性分类支持向量机是一种非常有效的方法。但有时分类问题是非线性的，这时就需要非线性支持向量机。

（**核函数**）设$\mathcal{X}$是输入空间，$\mathcal{H}$为特征空间，如果存在一个从输入空间$\mathcal{X}$到特征空间$\mathcal{H}$的映射：
$$
\phi(x):\mathcal{X} \rightarrow \mathcal{H}，
$$
使得对所有$x,z \in \mathcal{X}$，函数$K(x,z)$满足条件：
$$
K(x,z)=\phi(x)\cdot \phi(z),
$$
则称函数$K(x,z)$为核函数，$\phi(x)$为映射函数；式中$\phi(x)\cdot\phi(z)$为$\phi(x)$和$\phi(z)$的内积。

在核函数$K(x,z)$给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数，这样的技巧称为核技巧。它是巧妙地利用线性分类学习方法与核函数解决非线性问题的技术。

常用核函数

1. 多项式核函数
   $$
   K(x,z)=(x\cdot z + 1)^{p}
   $$

对应的支持向量机是一个$p$次多项式分类器。在此情形下，分类决策函数为：
$$
f(x)=\text{sign}[\sum_{i=1}^{N}\alpha_{i}^{*}y_i(x_i\cdot x+1)^p + b^{*} ]
$$

2. 高斯核函数
   $$
   K(x,z)=\exp(-\frac{\Vert x-z\Vert^2}{2\sigma^2})
   $$

对应的支持向量机是高斯径向基函数分类器。在此情况下，分类决策函数为：
$$
f(x)=\text{sign}[\sum_{i=1}^{N}\alpha_i^{*}y_i\exp(-\frac{\Vert x-z\Vert^2}{2\sigma^2}) + b^{*} ]
$$

3. 字符串核函数

   核函数不仅可以定义在欧式空间上，还可以定义在离散数据的集合上。例如，字符串核函数是定义在字符串集合上的核函数。字符串核函数在文本分类、信息检索、生物信息学等方便都有应用。

## 7.4 序列最小最优化算法

如何高效地实现支持向量机学习是一个很重要的问题。序列最小最优化（SMO）算法就是针对该问题而提出的。

SMO算法是支持向量机学习的一种快速算法，其特点是不断地将原二次规划问题分解为只有两个变量的二次规划子问题，并对子问题进行解析求解，直到所有变量满足KKT条件为止。这样通过启发式的方法得到原二次规划问题的最优解。因为子问题有解析解，所以每次计算子问题都很快，虽然子问题的个数很多，但该算法总体上还是高效的。