# 第四章　朴素贝叶斯算法

朴素贝叶斯算法是基于**贝叶斯定理**与**特征（属性）条件独立假设**的分类方法。该方法实现简单，学习与预测效率都很高，是一种常用的方法。

## 4.1 朴素贝叶斯方法的学习与分类

假设在训练数据集$T=\{(x_1, y_1), (x_2, y_2),\cdots , (x_N, y_N)  \}$中，每个样本中的$x_i\in \mathcal{X} $是一个$n$维特征向量，其对应的$y_i \in \mathcal{Y}$为其类别标记（标量），总共有$k$个类别，类别标记集合为$\{c_1, c_2, \cdots, c_k\}$。贝朴素贝叶斯方法通过训练数据集$T$学习其联合概率分布。具体学习的是先验概率分布和条件概率分布。先验概率分布：
$$
P(Y=c_j), \quad j=1,2,\cdots,k
$$
条件概率分布：
$$
P(X=x|Y=c_j)=P(x^{(1)},x^{(2)},\cdots,x^{(n)}|c_j)
$$
假设$x^{l}​$的可能取值有$S_l​$个，那么总的组合个数为$k\prod_{l=1}^{n}S_l​$，因此条件概率分布的参数组合具有指数级数量，故其估计实际是不可行的。

朴素贝叶斯方法对条件概率分布做了条件独立假设，即对于已知类别，假设所有特征相互独立。具体而言，条件独立假设为：
$$
P(x^{(1)},x^{(2)},\cdots,x^{(n)}|c_j)=\prod_{l=1}^{n}P(x^{(l)}| c_j)
$$
那么对于给定的特征向量$x​$，通过学习到的模型计算其后验概率，将后验概率最大的类作为$x​$的类输出：
$$
P(Y=c|X=x)=\frac{P(x,c)}{P(x)}=\frac{P(x|c)P(c)}{P(x)}=\frac{P(c)}{P(x)}\prod_{l=1}^{n}P(x^{(l)}| c)
$$
这是朴素贝叶斯方法分类的基本公式。由于$P(x)​$对于所有类别$c​$都是相同的，所以
$$
y=\arg\max_{c}P(c)\prod_{l=1}^{n}P(x^{(l)}|c)
$$
朴素贝叶斯算法将样本分到后验概率最大的类中，这等价于期望风险最小化，这是该算法采用的原理。

## 4.2 朴素贝叶斯的参数估计

在朴素贝叶斯算法中，需要估计类先验概率$P(c)$和条件概率$P(x^{(l)}|c)$，这里我们可以用**极大似然估计法**来估计相应的概率。类先验概率的极大似然估计是
$$
P(c)=\frac{|T_c|}{N}
$$
其中，$|T_c|$表示在训练数据集$T$中第$c$类样本的数量。条件概率的极大似然估计是
$$
P(x^{(l)}|c)=\frac{|T_{c,x^{(l)}}|}{|T_c|}
$$
其中，$|T_{c, x^{(l)}}|$表示在训练数据集$T$第$c$类样本在第$l$个特征取值为$x^{(l)}$的样本数量。

用上述公式去估计相应的概率，可能会产生所要估计的概率值为0的情况，这时会影响预测分类的结果。为了解决这个问题，常用“拉普拉斯修正（拉普拉斯平滑）”，具体来说，
$$
P(c)=\frac{|T_c|+1}{N+k} \\
P(x^{(l)}|c)=\frac{|T_{c,x^{(l)}}|+1}{|T_c|+S_{l}}
$$
其中，$k$和$S_l$分别对应上文的类别总数、第$l$个属性可能取值个数。

