# 第八章　提升方法

提升方法是一种常见的统计学习方法。在分类任务中，它通过改变训练数据集中的样本权重来学习多个分类器，并且将这些分类器进行线性组合，达到提升分类的性能的目的。

## 8.1 AdaBoost算法

提升方法是基于这样一种思想：对于一个复杂的任务来说，将多个专家的判断进行适当的综合所得到的判断，要比其中任何一个专家单独的判断好。最能代表这一思想的算法是*AdaBoost*算法。

大多数提升方法都是改变训练数据的概率分布，或者说训练数据的权值分布；针对不同训练数据的分布调用弱学习算法学习一系列的弱分类器；再将这些弱分类器构成一个强分类器。但是如何来改变训练数据的权重或概率分布；又如何将这些弱分类器组合成一个强分类器呢。

针对第一个问题，*AdaBoost*算法的解决办法是，提高前一轮那些被弱分类器错误分类样本的权重，在不影响正确率的情况下降低那些被正确分类样本的权重；针对第二个问题，*AdaBoost*算法采用了加权多数表决的方法，具体做法是加大分类误差率较小的弱分类器的权重，使其在表决中起较大的作用，减少分类误差率较大的弱分类器的权重，使其在表决中其较小的作用。

*AdaBoost*算法：

- 输入：训练数据集$T=\{(x_1, y_1), (x_2, y_2), \cdots , (x_N, y_N\}$，其中$x_i \in  R^n$，$y_i \in \{-1, +1\}$；以及弱学习算法；
1. 初始化训练数据集中的权值分布：
   $$
   D_0=(w_{0,1},w_{0,2},\cdots, w_{0,N}),\quad w_{0i}=\frac{1}{N},\quad i=1,2,\cdots,N
   $$

2. 对于$m=0,1,2,\cdots, M$，则有：

   1. 利用弱学习算法对具有权值分布为$D_{m}$的训练样本数据集进行学习，得到弱分类器$G_{m}(x)$；

   2. 依据弱分类器$G_{m}(x)​$，计算其在训练数据集上的分类误差率
      $$
      e_{m}=\sum_{i=1}^{N}P(G_{m}(x_i)\neq y_i)=\sum_{i=1}^{N}w_{m,i}I(G_{m}(x_i)\neq y_i)
      $$

   3. 然后计算弱分类器$G_{m}(x)$对应的权重系数
      $$
      \alpha_m=\frac{1}{2}\ln\frac{1-e_{m}}{e_m}
      $$

   4. 之后更新训练数据集的权重分布
      $$
      D_{m+1}=(w_{m+1,1},w_{m+1,2},\cdots,w_{m+1,N})\\
      w_{m+1,i}=\frac{w_{m,i}}{Z_m}\exp(-\alpha_{m}y_i G_m(x_i)),\quad i=1,2,\cdots,N
      $$
      其中$Z_{m}=\sum_{i=1}^{N}w_{m,i}\exp(-\alpha_{m}y_i G_m(x_i))$是规范化因子；

3. 构建弱分类器的线性组合$f(x)=\sum_{m=1}^{M}\alpha_{m} G_{m}(x)$，得到的强分类器为：
   $$
   G(x)=\text{sign}(f(x))=\text{sign}[\sum_{m=0}^{M}\alpha_{m}G_{m}(x) ]
   $$
- 输出：强分类器$G(x)$。

不改变所给的训练数据集，而不断改变训练数据集的权值分布，使得训练数据在基本分类器的学习中起到不同的作用，这是$AdaBoost$算法的一个特点；利用基本分类器的线性组合构成得到强分类器是它的另一个特点。

## 8.2 训练误差分析

*AdaBoost*算法最基本的性质是它能够在学习的过程中不断减少训练误差，即降低在训练数据集上的分类误差率，而且训练误差是以指数速率下降的。

*AdaBoost*算法的另外一个解释是，*AdaBoost*算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法的二分类学习方法。

## 8.3 提升树

提升树是以**分类树**或**回归树**为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。

### 8.3.1 提升树模型

提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。以决策树为基函数的提升方法称为提升树。提升树模型可以表示为决策树的加法模型：
$$
f_{M}(x)=\sum_{m=1}^{M}T(x;\Theta_{m}),
$$
其中$T(x;\Theta_{m})$表示决策树；$\Theta_{m}$是相应决策树的参数；$M$为决策树的个数。

### 8.3.2 提升树算法

提升树算法采用前向分步算法，首先初始化提升树$f_{0}(x)=0$，那么第$m$步的模型是
$$
f_{m}(x)=f_{m-1}(x)+T(x;\Theta_{m})
$$
其中$f_{m-1}(x)$为当前模型，通过经验风险最小化确定下一个决策树的参数$\Theta_{m}$，即：
$$
\hat{\Theta}_{m}=\arg\min_{\Theta_{m}}\sum_{i=1}^{N}L(y_i,f_{m}(x_i))
$$
即使训练数据中输入和输出之间的关系很复杂，但树的线性组合仍然可以很好地拟合训练数据。所以提升树是一个高功能的学习算法。

### 8.3.3 梯度提升

提升树是利用加法模型和前向分步算法实现学习、优化过程的。当损失函数是平方损失和指数损失函数时，每一步优化就简单了。但对于一般损失函数而言，每一步的优化并不容易。针对这个问题就提出了梯度提升算法。这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值
$$
-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}
$$
作为回归问题提升树算法中残差的近似值，用来拟合一个回归树。